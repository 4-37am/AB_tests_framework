{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cea08f-1ab6-475b-83f0-fb350de57d58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "\n",
    "# functions needed for further calculations:\n",
    "\n",
    "def relative_diff(num,denum):\n",
    "  if denum==0:\n",
    "    return np.NaN\n",
    "  elif denum<0:\n",
    "    return -(num/denum-1)\n",
    "  else:\n",
    "    return num/denum-1\n",
    "\n",
    "def array_diff(tr,ctr):\n",
    "  diff=np.array([])\n",
    "  if len(ctr[ctr<0])>0:\n",
    "    k=0\n",
    "    for c in ctr: \n",
    "      if c<0 and tr[k]<0:\n",
    "        diff=np.append(diff,-tr[k]+c)\n",
    "      else:\n",
    "        diff=np.append(diff,tr[k]-c)\n",
    "      k=k+1\n",
    "  else:\n",
    "    diff=tr-ctr\n",
    "  return diff\n",
    "\n",
    "# removes outliers with given quantile\n",
    "def remove_outliers(df_customer,metric,quantile_for_outliers):\n",
    "  outliers_threshold=df_customer[metric].astype('float').quantile(quantile_for_outliers)\n",
    "  df_wo_outliers = df_customer.drop(df_customer[(df_customer[metric] >= outliers_threshold)].index)\n",
    "  return df_wo_outliers,outliers_threshold\n",
    "\n",
    "# return samples based on column and metrics names\n",
    "def get_control_and_treatment_samples(df_customer,metric,variant_col,control_gr_name,treatment_gr_name):\n",
    "  control=df_customer[metric][df_customer[variant_col] == control_gr_name].values\n",
    "  control=control[~np.isnan(control)]\n",
    "  treatment=df_customer[metric][df_customer[variant_col] == treatment_gr_name].values\n",
    "  treatment=treatment[~np.isnan(treatment)]\n",
    "  return control, treatment\n",
    "\n",
    "# return z-test CI  \n",
    "def get_z_test_CI(control,treatment):\n",
    "  ci_diff_primary = 1.96 * np.sqrt(np.square(np.std(treatment)) / len(treatment) + np.square(np.std(control)) / len(control))\n",
    "  return ci_diff_primary\n",
    "\n",
    "# bootstrap\n",
    "def get_bootstrap_results(control,treatment,iterations=800,bootstrap_median=False):\n",
    "  sample_size=len(control)\n",
    "  boot_means_ctr=[]\n",
    "  boot_means_tr=[]\n",
    "  k = 0\n",
    "  boot_mean_diff = []\n",
    "  for b in np.arange(iterations):\n",
    "    bootstrap_controls=np.random.choice(control, size=sample_size, replace=True)\n",
    "    bootstrap_treatments=np.random.choice(treatment, size=sample_size, replace=True)\n",
    "    if bootstrap_median:\n",
    "      boot_mean_ctr=np.median(bootstrap_controls)\n",
    "      boot_mean_tr=np.median(bootstrap_treatments)\n",
    "    else:\n",
    "      boot_mean_ctr=np.mean(bootstrap_controls)\n",
    "      boot_mean_tr=np.mean(bootstrap_treatments)\n",
    "      \n",
    "    boot_means_ctr.append(boot_mean_ctr)\n",
    "    boot_means_tr.append(boot_mean_tr)\n",
    "    diff_means = boot_mean_tr - boot_mean_ctr\n",
    "    if diff_means<0:\n",
    "      k=k+1\n",
    "    boot_mean_diff.append(diff_means)\n",
    "  CI1_means = np.percentile(boot_mean_diff, [2.5, 97.5])\n",
    "  pval=2 * np.minimum(k, iterations - k) / iterations\n",
    "  ctr_mean=np.mean(boot_means_ctr)\n",
    "  tr_mean=np.mean(boot_means_tr)\n",
    "  mean_diff=relative_diff(tr_mean,ctr_mean)\n",
    "  ci_low=CI1_means[0]\n",
    "  ci_up=CI1_means[1]\n",
    "  if ctr_mean>0:\n",
    "    ci_prc_low=ci_low/ctr_mean\n",
    "    ci_prc_up=ci_up/ctr_mean\n",
    "  elif ctr_mean<0:\n",
    "    ci_prc_low=-ci_up/ctr_mean\n",
    "    ci_prc_up=-ci_low/ctr_mean\n",
    "  else:\n",
    "    ci_prc_low=0.0\n",
    "    ci_prc_up=0.0\n",
    "  return np.round(pval,4),np.round(ctr_mean,4),np.round(tr_mean,4),np.round(mean_diff,5),np.round(ci_prc_low,5),np.round(ci_prc_up,5)\n",
    "\n",
    "\n",
    "# get buckets and apply t_test\n",
    "def get_buckets_results(control,treatment,buckets=150,confidence_level = 0.95):\n",
    "  buckets_means_ctr=[]\n",
    "  buckets_means_tr=[]\n",
    "  means_diff=[]\n",
    "  control_available_buckets=control\n",
    "  treatment_available_buckets=treatment\n",
    "  sample_size=int(np.floor(len(control)/buckets))\n",
    "  if sample_size<1000:\n",
    "    buckets=int(np.floor(len(control)/1000))\n",
    "    sample_size=int(np.floor(len(control)/buckets))\n",
    "  \n",
    "  for b in np.arange(buckets):\n",
    "    if (sample_size<=len(control_available_buckets)) & (sample_size<=len(treatment_available_buckets)):\n",
    "      control_idx=np.random.randint(0, len(control_available_buckets), sample_size)\n",
    "      test_idx=np.random.randint(0, len(treatment_available_buckets), sample_size)\n",
    "      sample_ctr=control_available_buckets[control_idx]\n",
    "      sample_tr=treatment_available_buckets[test_idx]\n",
    "      tr_mean=np.mean(sample_tr)\n",
    "      ctr_mean=np.mean(sample_ctr)\n",
    "      means_diff.append(tr_mean-ctr_mean)\n",
    "      buckets_means_ctr.append(ctr_mean)\n",
    "      buckets_means_tr.append(tr_mean)\n",
    "      control_available_buckets = np.delete(control_available_buckets, control_idx)\n",
    "      treatment_available_buckets= np.delete(treatment_available_buckets, test_idx)\n",
    "\n",
    "  pval_mean=stats.ttest_ind(buckets_means_ctr,buckets_means_tr).pvalue\n",
    "  mean_buckets_tr=np.mean(buckets_means_tr)\n",
    "  mean_buckets_ctr=np.mean(buckets_means_ctr)\n",
    "  \n",
    "  degrees_freedom = len(means_diff) - 1\n",
    "  sample_mean = np.mean(means_diff)\n",
    "  sample_standard_error = stats.sem(means_diff)\n",
    "  confidence_interval = stats.t.interval(confidence_level, degrees_freedom, sample_mean, sample_standard_error)\n",
    "  ci_low=confidence_interval[0]\n",
    "  ci_up=confidence_interval[1]\n",
    "  if mean_ctr>0:\n",
    "    ci_prc_low=ci_low/mean_buckets_ctr\n",
    "    ci_prc_up=ci_up/mean_buckets_ctr\n",
    "  elif mean_ctr<0:\n",
    "    ci_prc_low=-ci_up/mean_buckets_ctr\n",
    "    ci_prc_up=-ci_low/mean_buckets_ctr\n",
    "  else:\n",
    "    ci_prc_low=0.0\n",
    "    ci_prc_up=0.0\n",
    "  \n",
    "   \n",
    "  return np.round(pval_mean,4),np.round(mean_buckets_ctr,4),np.round(mean_buckets_tr,4),np.round(relative_diff(mean_buckets_tr,mean_buckets_ctr), 5),np.round(ci_prc_low,5),np.round(ci_prc_up,5)\n",
    "\n",
    "\n",
    "# return t-test pval,CIs, mean diffs\n",
    "def get_t_test_results(ctr,tr,confidence_level = 0.95):\n",
    "  ctr=ctr.astype('float')\n",
    "  tr=tr.astype('float')\n",
    "  pval=stats.ttest_ind(ctr,tr).pvalue\n",
    "  mean_tr=np.mean(tr)\n",
    "  mean_ctr=np.mean(ctr)\n",
    "  if len(tr)>len(ctr):\n",
    "    tr=np.random.choice(tr, size=len(ctr), replace=False)\n",
    "  else:\n",
    "    ctr=np.random.choice(ctr, size=len(tr), replace=False)\n",
    "  diff=array_diff(tr,ctr)\n",
    "  degrees_freedom = len(diff) - 1\n",
    "  sample_mean = np.mean(diff)\n",
    "  sample_standard_error = stats.sem(diff)\n",
    "  confidence_interval = stats.t.interval(confidence_level, degrees_freedom, sample_mean, sample_standard_error)\n",
    "  ci_low=confidence_interval[0]\n",
    "  ci_up=confidence_interval[1]\n",
    "  if mean_ctr>0:\n",
    "    ci_prc_low=ci_low/mean_ctr\n",
    "    ci_prc_up=ci_up/mean_ctr\n",
    "  elif mean_ctr<0:\n",
    "    ci_prc_low=-ci_up/mean_ctr\n",
    "    ci_prc_up=-ci_low/mean_ctr\n",
    "  else:\n",
    "    ci_prc_low=0.0\n",
    "    ci_prc_up=0.0\n",
    "  \n",
    "  return np.round(pval,4),np.round(mean_tr,4),np.round(mean_ctr,4),np.round(relative_diff(mean_tr,mean_ctr),5),np.round(ci_prc_low,5),np.round(ci_prc_up,5)\n",
    "\n",
    "# get results of proprtion test\n",
    "def get_chi2_test_results(control_num,control_denum,treatment_num,treatment_denum):\n",
    "  sum_control_num = np.sum(control_num)\n",
    "  sum_control_denum = np.sum(control_denum)\n",
    "  sum_treatment_num = np.sum(treatment_num)\n",
    "  sum_treatment_denum = np.sum(treatment_denum)\n",
    "  p_control = sum_control_num/sum_control_denum\n",
    "  p_treatment = sum_treatment_num/sum_treatment_denum\n",
    "  p_prc_diff = np.round(((p_treatment/p_control)-1),4)\n",
    "  p_diff = p_treatment - p_control\n",
    "  p_control = np.round(p_control, 4)\n",
    "  p_treatment = np.round(p_treatment, 4)\n",
    "  T = np.array([[sum_control_num, sum_control_denum - sum_control_num], [sum_treatment_num, sum_treatment_denum - sum_treatment_num]])\n",
    "  conv_pval=np.round(stats.chi2_contingency(T,correction=False)[1],5)\n",
    "  ci_low,ci_up=confint_proportions_2indep(sum_treatment_num,sum_treatment_denum,sum_control_num,sum_control_denum,alpha=0.05,compare='diff')\n",
    "  ci_prc_low=np.round(ci_low/p_control,4)\n",
    "  ci_prc_up=np.round(ci_up/p_control,4)\n",
    " \n",
    "\n",
    "  return conv_pval, ci_low, ci_up, ci_prc_low, ci_prc_up, p_control, p_treatment, p_diff, p_prc_diff\n",
    "\n",
    "def get_bootstrap_results_for_conversions(control_num_arr, control_denum_arr, tr_num_arr, tr_denum_arr, bootstrap_iterations=800):\n",
    "  bootstrap_sample_size=len(control_denum_arr)\n",
    "  boot_means_ctr=[]\n",
    "  boot_means_tr=[]\n",
    "  boot_mean_diff=[]\n",
    "  k=0\n",
    "  for b in np.arange(bootstrap_iterations):\n",
    "    idx_control=np.random.choice(np.arange(len(control_denum_arr)), size=bootstrap_sample_size, replace=True)\n",
    "    control_denum=np.sum(control_denum_arr[idx_control])\n",
    "    control_num=np.sum(control_num_arr[idx_control])\n",
    "    idx_tr=np.random.choice(np.arange(len(tr_denum_arr)), size=bootstrap_sample_size, replace=True)\n",
    "    tr_denum=np.sum(tr_denum_arr[idx_tr])\n",
    "    tr_num=np.sum(tr_num_arr[idx_tr])\n",
    "    boot_mean_ctr=control_num/control_denum\n",
    "    boot_mean_tr=tr_num/tr_denum\n",
    "    boot_means_ctr.append(control_num/control_denum)\n",
    "    boot_means_tr.append(tr_num/tr_denum)  \n",
    "    diff_means = boot_mean_tr - boot_mean_ctr\n",
    "    if diff_means<0:\n",
    "      k=k+1\n",
    "    boot_mean_diff.append(diff_means)\n",
    "  CI1_means = np.percentile(boot_mean_diff, [2.5, 97.5])\n",
    "  pval=2 * np.minimum(k, bootstrap_iterations - k) / bootstrap_iterations\n",
    "  ctr_mean=np.mean(boot_means_ctr)\n",
    "  tr_mean=np.mean(boot_means_tr)\n",
    "  mean_diff=relative_diff(tr_mean,ctr_mean)\n",
    "  ci_low=CI1_means[0]\n",
    "  ci_up=CI1_means[1]\n",
    "  ci_prc_low=ci_low/ctr_mean\n",
    "  ci_prc_up=ci_up/ctr_mean\n",
    "  return np.round(pval,4),np.round(ctr_mean,4),np.round(tr_mean,4),np.round(mean_diff,5),np.round(ci_prc_low,5),np.round(ci_prc_up,5)\n",
    "  \n",
    "def get_buckets_results_for_conversions(control_num_arr, control_denum_arr, tr_num_arr, tr_denum_arr,buckets=150,confidence_level = 0.95):\n",
    "  buckets_means_ctr=[]\n",
    "  buckets_means_tr=[]\n",
    "  means_diff=[]\n",
    "  control_num_available_buckets=control_num_arr\n",
    "  treatment_num_available_buckets=tr_num_arr\n",
    "  control_denum_available_buckets=control_denum_arr\n",
    "  treatment_denum_available_buckets=tr_denum_arr\n",
    "  sample_size=int(np.floor(len(control_num_arr)/buckets))\n",
    "  if sample_size<4000:\n",
    "    buckets=int(np.floor(len(control_num_arr)/4000))\n",
    "    sample_size=int(np.floor(len(control_num_arr)/buckets))\n",
    "  \n",
    "  for b in np.arange(buckets):\n",
    "    if (sample_size<=len(control_num_available_buckets)) & (sample_size<=len(treatment_num_available_buckets)):\n",
    "      control_idx=np.random.randint(0, len(control_num_available_buckets), sample_size)\n",
    "      test_idx=np.random.randint(0, len(treatment_num_available_buckets), sample_size)\n",
    "      sample_ctr=control_num_available_buckets[control_idx]/control_denum_available_buckets[control_idx]\n",
    "      sample_tr=treatment_num_available_buckets[test_idx]/treatment_denum_available_buckets[test_idx]\n",
    "      tr_mean=np.mean(sample_tr)\n",
    "      ctr_mean=np.mean(sample_ctr)\n",
    "      means_diff.append(tr_mean-ctr_mean)\n",
    "      buckets_means_ctr.append(ctr_mean)\n",
    "      buckets_means_tr.append(tr_mean)\n",
    "      control_num_available_buckets = np.delete(control_num_available_buckets, control_idx)\n",
    "      treatment_num_available_buckets= np.delete(treatment_num_available_buckets, test_idx)\n",
    "      control_denum_available_buckets = np.delete(control_denum_available_buckets, control_idx)\n",
    "      treatment_denum_available_buckets= np.delete(treatment_denum_available_buckets, test_idx)\n",
    "\n",
    "  pval_mean=stats.ttest_ind(buckets_means_ctr,buckets_means_tr).pvalue\n",
    "  mean_buckets_tr=np.mean(buckets_means_tr)\n",
    "  mean_buckets_ctr=np.mean(buckets_means_ctr)\n",
    "  \n",
    "  \n",
    "  degrees_freedom = len(means_diff) - 1\n",
    "  sample_mean = np.mean(means_diff)\n",
    "  sample_standard_error = stats.sem(means_diff)\n",
    "  confidence_interval = stats.t.interval(confidence_level, degrees_freedom, sample_mean, sample_standard_error)\n",
    "  ci_low=confidence_interval[0]\n",
    "  ci_up=confidence_interval[1]\n",
    "  ci_prc_low=ci_low/mean_buckets_ctr\n",
    "  ci_prc_up=ci_up/mean_buckets_ctr\n",
    "  \n",
    "   \n",
    "  return np.round(pval_mean,4),np.round(mean_buckets_ctr,4),np.round(mean_buckets_tr,4),np.round(relative_diff(mean_buckets_tr,mean_buckets_ctr), 5), np.round(ci_prc_low,5), np.round(ci_prc_up,5) \n",
    "\n",
    "\n",
    "def get_bootstrap_results_for_conversions_mde(control_num_arr, control_denum_arr, uplift=0.0, bootstrap_iterations=800):\n",
    "  bootstrap_sample_size=len(control_denum_arr)\n",
    "  boot_means_ctr=[]\n",
    "  boot_means_tr=[]\n",
    "  boot_mean_diff=[]\n",
    "  k=0\n",
    "  for b in np.arange(bootstrap_iterations):\n",
    "    idx_control=np.random.choice(np.arange(len(control_denum_arr)), size=bootstrap_sample_size, replace=True)\n",
    "    control_denum=np.sum(control_denum_arr[idx_control])\n",
    "    control_num=np.sum(control_num_arr[idx_control])\n",
    "    idx_tr=np.random.choice(np.arange(len(control_denum_arr)), size=bootstrap_sample_size, replace=True)\n",
    "    tr_denum=np.sum(control_denum_arr[idx_tr])\n",
    "    tr_num=np.sum(control_num_arr[idx_tr])\n",
    "    boot_mean_ctr=control_num/control_denum\n",
    "    boot_mean_tr=tr_num/tr_denum*(1+uplift)\n",
    "    diff_means = boot_mean_tr - boot_mean_ctr\n",
    "    if diff_means<0:\n",
    "      k=k+1\n",
    "  pval=2 * np.minimum(k, bootstrap_iterations - k) / bootstrap_iterations\n",
    "  return np.round(pval,4)\n",
    "  \n",
    "def get_mde(pval, mean_diff, ci_low, ci_up):\n",
    "  mde = 1.0\n",
    "  if pval<=0.05 and mean_diff>=0:\n",
    "    mde = mean_diff - ci_low\n",
    "  elif pval<=0.05 and mean_diff<0:\n",
    "    mde = abs(mean_diff) - abs(ci_up)\n",
    "  elif pval>0.05 and mean_diff>=0:\n",
    "    mde = abs(mean_diff)+abs(ci_low)\n",
    "  elif pval>0.05 and mean_diff<0:\n",
    "    mde = abs(mean_diff)+abs(ci_up)\n",
    "  return np.round(mde,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa97686-aed8-4b6d-b86a-a510d3ab4a37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main_ab_test_calculation_function(customer_id_column, test_start_date, day, calc_start_date, calc_end_date, metrics_columns, need_bootstrap, need_buckets, proportions_metrics_flag, proportions_columns,ratio_metrics_flag,ratio_columns, query , is_zps_kpis_list_needed = True\n",
    "  , query_flat=None):\n",
    "  \n",
    "  \n",
    "  metrics_columns_internal = [x for x in metrics_columns]\n",
    "  \n",
    "  df_results=pd.DataFrame()\n",
    "  print(day)\n",
    "  start_time = time.time()\n",
    "\n",
    "  df_main_query = dwhRead(query)\n",
    "  df_main_query = df_main_query.toPandas()\n",
    "#   calculate ZPS KPIs\n",
    "  zps_kpis_query= f\"\"\"\n",
    "    select customer_id\n",
    "         , coalesce(sum(gmv_bef_cancellation), 0)::float as zps_kpi_gmvbr\n",
    "         , coalesce(count(purchase_attempt_id), 0) as zps_kpi_num_purchase_attempts\n",
    "         , coalesce(sum(num_orders_placed), 0) as zps_kpi_num_orders_placed\n",
    "         , zps_kpi_num_orders_placed as zps_kpi_num_orders_placed_2\n",
    "         , coalesce(count(case when funnel_saw_payment_page then purchase_attempt_id end), 0) as zps_kpi_num_purchase_attempts_with_payment_methods_rendered\n",
    "         , coalesce(count(case when funnel_reached_payment_selected and funnel_saw_payment_page then purchase_attempt_id end), 0) as zps_kpi_num_purchase_attempts_reached_payment_selected_with_rendered\n",
    "         , coalesce(count(case when funnel_reached_payment_initiated then purchase_attempt_id end), 0) as zps_kpi_num_purchase_attempts_reached_payment_initiated\n",
    "    from reporting.pam_purchase_attempt_funnel\n",
    "    where purchase_attempt_created_at::date between '\"\"\"+test_start_date+\"\"\"' and '\"\"\"+day+\"\"\"'\n",
    "    group by 1\n",
    "  \"\"\"\n",
    "  if is_zps_kpis_list_needed:\n",
    "    df_zps_kpis = dwhRead(zps_kpis_query)\n",
    "    df_zps_kpis = df_zps_kpis.toPandas()\n",
    "    \n",
    "    df_customer = df_main_query.merge(df_zps_kpis, left_on=customer_id_column, right_on='customer_id', how='left')\n",
    "  else:\n",
    "    df_customer = df_main_query\n",
    "  \n",
    "  if query_flat:\n",
    "    df_customer_flat = dwhRead(query_flat).toPandas()\n",
    "      \n",
    "    flat_metrics = df_customer_flat.metric_name.unique().tolist()\n",
    "    main_metrics = df_customer.columns.tolist()\n",
    "    \n",
    "    metrics_columns_internal += flat_metrics\n",
    "    \n",
    "    df_customer[customer_id_column]=df_customer[customer_id_column].astype(str)\n",
    "    df_customer_flat[customer_id_column]=df_customer_flat[customer_id_column].astype(str)\n",
    "    \n",
    "    for metric in flat_metrics:\n",
    "      if metric in main_metrics:\n",
    "          continue\n",
    "      res = df_customer_flat[df_customer_flat.metric_name == metric][[customer_id_column, 'metric_value']].rename(columns={'metric_value': metric})\n",
    "      df_customer = df_customer.merge(res, on=customer_id_column, how='left')\n",
    "       \n",
    "  print (list(df_customer.columns))\n",
    "  print(\" %s minutes needed for Query calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "  start_time = time.time()\n",
    "#   inject ZPS KPIs to metrics list\n",
    "  if is_zps_kpis_list_needed:\n",
    "    for kpi in ['zps_kpi_gmvbr','zps_kpi_num_purchase_attempts','zps_kpi_num_orders_placed']:\n",
    "      metrics_columns_internal.append(kpi)\n",
    "      \n",
    "  for metric in metrics_columns_internal:\n",
    "    is_median_needed=False\n",
    "    metric_type = 'none'\n",
    "    if type(metric) is dict:\n",
    "      metric_name=list(metric.keys())[0]\n",
    "      metric_type=metric.get(metric_name)\n",
    "      metric=metric_name\n",
    "      print (metric_name,metric_type,'start calculations' )\n",
    "    else: print(metric,'start calculations')\n",
    "    if metric_type.lower() == 'median':\n",
    "      is_median_needed=True\n",
    "    #     remove outliers \n",
    "    df_wo_outliers,outliers_threshold = remove_outliers(df_customer,metric,quantile_for_outliers)\n",
    "\n",
    "    #     get needed data from DF\n",
    "    control,treatment = get_control_and_treatment_samples(df_customer,metric,variant_col,control_gr_name,treatment_gr_name)\n",
    "    \n",
    "    if not is_median_needed:\n",
    "      control_wo_outliers,treatment_wo_outliers = get_control_and_treatment_samples(df_wo_outliers,metric,variant_col,control_gr_name,treatment_gr_name)\n",
    "    #     T-test pvalue calculations\n",
    "      ttest_pval, ttest_mean_tr, ttest_mean_ctr, ttest_mean_diff, ttest_ci_prc_low, ttest_ci_prc_up = get_t_test_results(control,treatment)\n",
    "      ttest_pval_wo_outliers, ttest_mean_tr_wo_outliers, ttest_mean_ctr_wo_outliers, ttest_mean_diff_wo_outliers, ttest_ci_prc_low_wo_outliers, ttest_ci_prc_up_wo_outliers = get_t_test_results(control_wo_outliers,treatment_wo_outliers)\n",
    "\n",
    "    #  MW test calculations\n",
    "      mw_test_pval=stats.mannwhitneyu(control,treatment).pvalue\n",
    "\n",
    "    #  Mean, median and its differences\n",
    "      control_mean=np.round(control.mean(),4)\n",
    "      treatment_mean=np.round(treatment.mean(),4)\n",
    "      mean_diff=np.round(relative_diff(treatment_mean,control_mean),4)\n",
    "\n",
    "      control_mean_wo_outliers=np.round(control_wo_outliers.mean(),4)\n",
    "      treatment_mean_wo_outliers=np.round(treatment_wo_outliers.mean(),4)\n",
    "      mean_diff_wo_outliers=np.round(relative_diff(treatment_mean_wo_outliers,control_mean_wo_outliers),4)\n",
    "    else:\n",
    "      ttest_pval = np.NaN \n",
    "      ttest_mean_tr = np.NaN \n",
    "      ttest_mean_ctr  = np.NaN\n",
    "      ttest_mean_diff = np.NaN\n",
    "      ttest_ci_prc_low = np.NaN\n",
    "      ttest_ci_prc_up = np.NaN\n",
    "      ttest_pval_wo_outliers = np.NaN\n",
    "      ttest_mean_tr_wo_outliers = np.NaN \n",
    "      ttest_mean_ctr_wo_outliers = np.NaN \n",
    "      ttest_mean_diff_wo_outliers = np.NaN\n",
    "      ttest_ci_prc_low_wo_outliers = np.NaN\n",
    "      ttest_ci_prc_up_wo_outliers = np.NaN\n",
    "      \n",
    "\n",
    "    control_median=np.median(control)\n",
    "    treatment_median=np.median(treatment)\n",
    "    median_diff=np.round(relative_diff(treatment_median,control_median),4)\n",
    "\n",
    "#     #     CI bounds calculation\n",
    "#     ci_diff_primary = get_z_test_CI(control,treatment)\n",
    "#     ci_diff_primary_wo_outliers = get_z_test_CI(control_wo_outliers,treatment_wo_outliers)\n",
    "\n",
    "    print(\" %s minutes needed for usual tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "    # bootstrap\n",
    "    if need_bootstrap:\n",
    "        btstrp_pval, btstrp_ctr_mean, btstrp_tr_mean, btstrp_mean_diff, btstrp_ci_prc_low, btstrp_ci_prc_up = get_bootstrap_results(control,treatment,bootstrap_iterations, bootstrap_median = is_median_needed)\n",
    "    else:\n",
    "      btstrp_pval = np.NaN\n",
    "      btstrp_ctr_mean = np.NaN\n",
    "      btstrp_tr_mean = np.NaN\n",
    "      btstrp_mean_diff = np.NaN\n",
    "      btstrp_ci_prc_low = np.NaN\n",
    "      btstrp_ci_prc_up = np.NaN\n",
    "    if need_boostrap_wo_outliers and not is_median_needed:\n",
    "      btstrp_pval_wo_outliers, btstrp_ctr_mean_wo_outliers, btstrp_tr_mean_wo_outliers, btstrp_mean_diff_wo_outliers, btstrp_ci_prc_low_wo_outliers, btstrp_ci_prc_up_wo_outliers = get_bootstrap_results(control_wo_outliers,treatment_wo_outliers,bootstrap_iterations)\n",
    "    else:\n",
    "      btstrp_pval_wo_outliers = np.NaN \n",
    "      btstrp_ctr_mean_wo_outliers  = np.NaN \n",
    "      btstrp_tr_mean_wo_outliers  = np.NaN\n",
    "      btstrp_mean_diff_wo_outliers = np.NaN\n",
    "      btstrp_ci_prc_low_wo_outliers = np.NaN\n",
    "      btstrp_ci_prc_up_wo_outliers  = np.NaN\n",
    "\n",
    "\n",
    "\n",
    "    print(\" %s minutes needed for usual+bootstrap tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "    # buckets+t_test\n",
    "    if need_buckets:\n",
    "      buckets_pval, buckets_ctr_mean, buckets_tr_mean, buckets_mean_diff, buckets_ci_prc_low, buckets_ci_prc_up = get_buckets_results(control,treatment,buckets_num)\n",
    "    else:\n",
    "      buckets_pval = np.NaN\n",
    "      buckets_ctr_mean = np.NaN\n",
    "      buckets_tr_mean = np.NaN\n",
    "      buckets_mean_diff = np.NaN\n",
    "      buckets_ci_prc_low = np.NaN\n",
    "      buckets_ci_prc_up = np.NaN\n",
    "    if need_buckets_wo_outliers:\n",
    "      buckets_pval_wo_outliers, buckets_ctr_mean_wo_outliers, buckets_tr_mean_wo_outliers, buckets_mean_diff_wo_outliers, buckets_ci_prc_low_wo_outliers, buckets_ci_prc_up_wo_outliers = get_buckets_results(control_wo_outliers,treatment_wo_outliers,buckets_num)\n",
    "    else:\n",
    "      buckets_pval_wo_outliers = np.NaN\n",
    "      buckets_ctr_mean_wo_outliers = np.NaN\n",
    "      buckets_tr_mean_wo_outliers = np.NaN\n",
    "      buckets_mean_diff_wo_outliers = np.NaN\n",
    "      buckets_ci_prc_low_wo_outliers = np.NaN\n",
    "      buckets_ci_prc_up_wo_outliers = np.NaN\n",
    "\n",
    "    print(\" %s minutes needed for usual+bootstrap+buckets tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "    btstrp_mde = get_mde(btstrp_pval, mean_diff, btstrp_ci_prc_low, btstrp_ci_prc_up)\n",
    "    ttest_wo_outliers_mde = get_mde(ttest_pval, mean_diff, ttest_ci_prc_low, ttest_ci_prc_up)\n",
    "\n",
    "    print(\" %s minutes needed for usual+bootstrap+buckets+mde tests calculations \" % (np.round((time.time() - start_time)/60,1)))    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     collect results and append it to results df\n",
    "    if is_median_needed:\n",
    "      metric = metric + '_median'\n",
    "      control_mean = control_median\n",
    "      treatment_mean = treatment_median\n",
    "      mean_diff = median_diff\n",
    "           \n",
    "    results_for_day = pd.Series(data={'experiment_name':experiment_name,'dt':day,'metric':metric\n",
    "           ,'metric_type': 'continuous'\n",
    "           ,'mean_control':control_mean\n",
    "           ,'mean_treatment':treatment_mean\n",
    "           ,'mean_diff':mean_diff\n",
    "           ,'mean_control_wo_outliers':control_mean_wo_outliers\n",
    "           ,'mean_treatment_wo_outliers':treatment_mean_wo_outliers\n",
    "           ,'mean_diff_wo_outliers':mean_diff_wo_outliers\n",
    "           ,'median_control': control_median\n",
    "           ,'median_treatment':treatment_median\n",
    "           ,'median_diff':median_diff\n",
    "           ,'ttest_pval':ttest_pval\n",
    "           ,'ttest_mean_diff':ttest_mean_diff\n",
    "           ,'ttest_ci_prc_low':ttest_ci_prc_low\n",
    "           ,'ttest_ci_prc_up':ttest_ci_prc_up\n",
    "           ,'ttest_wo_outliers_pval':ttest_pval_wo_outliers\n",
    "           ,'ttest_wo_outliers_mean_diff':ttest_mean_diff_wo_outliers\n",
    "           ,'ttest_wo_outliers_ci_prc_low':ttest_ci_prc_low_wo_outliers\n",
    "           ,'ttest_wo_outliers_ci_prc_up':ttest_ci_prc_up_wo_outliers\n",
    "           ,'btstrp_pval':btstrp_pval\n",
    "           ,'btstrp_mean_diff':btstrp_mean_diff\n",
    "           ,'btstrp_ci_prc_low':btstrp_ci_prc_low\n",
    "           ,'btstrp_ci_prc_up':btstrp_ci_prc_up\n",
    "           ,'btstrp_wo_outliers_pval':btstrp_pval_wo_outliers\n",
    "           ,'btstrp_wo_outliers_mean_diff':btstrp_mean_diff_wo_outliers\n",
    "           ,'btstrp_wo_outliers_ci_prc_low':btstrp_ci_prc_low_wo_outliers\n",
    "           ,'btstrp_wo_outliers_ci_prc_up':btstrp_ci_prc_up_wo_outliers\n",
    "           ,'buckets_pval':buckets_pval\n",
    "           ,'buckets_mean_diff':buckets_mean_diff\n",
    "           ,'buckets_ci_prc_low':buckets_ci_prc_low\n",
    "           ,'buckets_ci_prc_up':buckets_ci_prc_up\n",
    "           ,'buckets_wo_outliers_pval':buckets_pval_wo_outliers\n",
    "           ,'buckets_wo_outliers_mean_diff':buckets_mean_diff_wo_outliers\n",
    "           ,'buckets_wo_outliers_ci_prc_low':buckets_ci_prc_low_wo_outliers\n",
    "           ,'buckets_wo_outliers_ci_prc_up':buckets_ci_prc_up_wo_outliers\n",
    "           ,'MW_pval':mw_test_pval\n",
    "           ,'chi2_pval':np.NaN                         \n",
    "           ,'outliers_percentile':quantile_for_outliers\n",
    "           ,'outliers_threshold':outliers_threshold\n",
    "           ,'calculations_time':calculations_time\n",
    "           ,'mde_bootstrap':btstrp_mde\n",
    "           ,'mde_ttest_wo_outliers':ttest_wo_outliers_mde\n",
    "           ,'units_control': len(control)\n",
    "           ,'units_treatment': len(treatment)\n",
    "           ,'srm_pval': np.round(stats.chisquare([len(control),len(treatment)])[1],4)\n",
    "          },name=calculations_time)\n",
    "    df_results=df_results.append(results_for_day, ignore_index=False)\n",
    "  if proportions_metrics_flag:\n",
    "    for num_col in proportions_columns.keys():\n",
    "      denum_col=proportions_columns[num_col]\n",
    "      control_num,treatment_num = get_control_and_treatment_samples(df_customer,num_col,variant_col,control_gr_name,treatment_gr_name)\n",
    "      control_denum,treatment_denum = get_control_and_treatment_samples(df_customer,denum_col,variant_col,control_gr_name,treatment_gr_name)\n",
    "      conv_pval, ci_low, ci_up, ci_prc_low, ci_prc_up, proportions_control, proportions_treatment, p_diff, proportions_diff = get_chi2_test_results(control_num,control_denum,treatment_num,treatment_denum)\n",
    "\n",
    "      ztest_mde = get_mde(conv_pval, proportions_diff, ci_prc_low, ci_prc_up)\n",
    "\n",
    "      results_for_day = pd.Series(data={'experiment_name':experiment_name,'dt':day\n",
    "           ,'metric':(num_col+'/'+denum_col)\n",
    "           ,'metric_type': 'proportion'\n",
    "           ,'mean_control':proportions_control\n",
    "           ,'mean_treatment':proportions_treatment\n",
    "           ,'mean_diff':proportions_diff\n",
    "           ,'mean_control_wo_outliers':np.NaN\n",
    "           ,'mean_treatment_wo_outliers':np.NaN\n",
    "           ,'mean_diff_wo_outliers':np.NaN\n",
    "           ,'median_control': np.NaN\n",
    "           ,'median_treatment':np.NaN\n",
    "           ,'median_diff':np.NaN\n",
    "           ,'ttest_pval':np.NaN\n",
    "           ,'ttest_mean_diff':np.NaN\n",
    "           ,'ttest_ci_prc_low':np.NaN\n",
    "           ,'ttest_ci_prc_up':np.NaN\n",
    "           ,'ttest_wo_outliers_pval':np.NaN\n",
    "           ,'ttest_wo_outliers_mean_diff':np.NaN\n",
    "           ,'ttest_wo_outliers_ci_prc_low':np.NaN\n",
    "           ,'ttest_wo_outliers_ci_prc_up':np.NaN\n",
    "           ,'btstrp_pval':np.NaN\n",
    "           ,'btstrp_mean_diff':np.NaN\n",
    "           ,'btstrp_ci_prc_low':ci_prc_low\n",
    "           ,'btstrp_ci_prc_up':ci_prc_up\n",
    "           ,'btstrp_wo_outliers_pval':np.NaN\n",
    "           ,'btstrp_wo_outliers_mean_diff':np.NaN\n",
    "           ,'btstrp_wo_outliers_ci_prc_low':np.NaN\n",
    "           ,'btstrp_wo_outliers_ci_prc_up':np.NaN\n",
    "           ,'buckets_pval':np.NaN\n",
    "           ,'buckets_mean_diff':np.NaN\n",
    "           ,'buckets_ci_prc_low':np.NaN\n",
    "           ,'buckets_ci_prc_up':np.NaN\n",
    "           ,'buckets_wo_outliers_pval':np.NaN\n",
    "           ,'buckets_wo_outliers_mean_diff':np.NaN\n",
    "           ,'buckets_wo_outliers_ci_prc_low':np.NaN\n",
    "           ,'buckets_wo_outliers_ci_prc_up':np.NaN\n",
    "           ,'MW_pval':np.NaN\n",
    "           ,'chi2_pval':conv_pval                         \n",
    "           ,'outliers_percentile':np.NaN\n",
    "           ,'outliers_threshold':np.NaN\n",
    "           ,'calculations_time':calculations_time\n",
    "           ,'mde_bootstrap':ztest_mde\n",
    "           ,'mde_ttest_wo_outliers':np.NaN\n",
    "           ,'units_control': np.sum(control_denum)\n",
    "           ,'units_treatment': np.sum(treatment_denum)\n",
    "           ,'srm_pval': np.round(stats.chisquare([np.sum(control_denum),np.sum(treatment_denum)])[1],4)                           \n",
    "          },name=calculations_time)\n",
    "      df_results=df_results.append(results_for_day, ignore_index=False)\n",
    "      print(\" %s minutes needed for proportions usual tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "#       inject ZPS KPIs ratio metrics\n",
    "  if is_zps_kpis_list_needed:\n",
    "    if not ratio_metrics_flag:\n",
    "      ratio_columns = {}\n",
    "    ratio_columns['zps_kpi_num_purchase_attempts_reached_payment_selected_with_rendered'] = 'zps_kpi_num_purchase_attempts_with_payment_methods_rendered'\n",
    "    ratio_columns['zps_kpi_num_orders_placed'] = 'zps_kpi_num_purchase_attempts_reached_payment_initiated'\n",
    "    ratio_columns['zps_kpi_num_orders_placed_2'] = 'zps_kpi_num_purchase_attempts'\n",
    "    \n",
    "  if ratio_metrics_flag or is_zps_kpis_list_needed:\n",
    "    for num_col in ratio_columns.keys():\n",
    "      print(num_col,'start calculations')\n",
    "      denum_col=ratio_columns[num_col]\n",
    "      control_num,treatment_num = get_control_and_treatment_samples(df_customer,num_col,variant_col,control_gr_name,treatment_gr_name)\n",
    "      control_denum,treatment_denum = get_control_and_treatment_samples(df_customer,denum_col,variant_col,control_gr_name,treatment_gr_name)\n",
    "      ratio_control=np.round(np.sum(control_num)/np.sum(control_denum),4)\n",
    "      ratio_treatment=np.round(np.sum(treatment_num)/np.sum(treatment_denum),4)\n",
    "      ratio_diff= np.round(relative_diff(ratio_treatment,ratio_control),4)\n",
    "      btstrp_pval, btstrp_ctr_mean, btstrp_tr_mean, btstrp_mean_diff, btstrp_ci_prc_low, btstrp_ci_prc_up = get_bootstrap_results_for_conversions(control_num, control_denum, treatment_num, treatment_denum, bootstrap_iterations)\n",
    "      print(\" %s minutes needed for ratio bootstrap tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "#       bucket test if requested\n",
    "      if need_buckets:\n",
    "        buckets_pval, buckets_ctr_mean, buckets_tr_mean, buckets_mean_diff, buckets_ci_prc_low, buckets_ci_prc_up = get_buckets_results_for_conversions(control_num, control_denum, treatment_num, treatment_denum, buckets_num)\n",
    "      else:\n",
    "        buckets_pval = np.NaN\n",
    "        buckets_ctr_mean = np.NaN\n",
    "        buckets_tr_mean = np.NaN\n",
    "        buckets_mean_diff = np.NaN\n",
    "        buckets_ci_prc_low = np.NaN\n",
    "        buckets_ci_prc_up = np.NaN\n",
    "      print(\" %s minutes needed for ratio bootstrap+buckets tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "      btstrp_mde = get_mde(btstrp_pval, ratio_diff, btstrp_ci_prc_low, btstrp_ci_prc_up)\n",
    "\n",
    "      print(\" %s minutes needed for ratio bootstrap+buckets and MDE tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "\n",
    "#       inject readable name for ZPS KPIs PACR, PPSR, PSSR\n",
    "      if num_col == 'zps_kpi_num_purchase_attempts_reached_payment_selected_with_rendered' and denum_col == 'zps_kpi_num_purchase_attempts_with_payment_methods_rendered':\n",
    "        ratio_metric_name = 'zps_kpi_PSSR'\n",
    "      elif num_col == 'zps_kpi_num_orders_placed' and denum_col == 'zps_kpi_num_purchase_attempts_reached_payment_initiated':\n",
    "        ratio_metric_name = 'zps_kpi_PPSR'\n",
    "      elif num_col == 'zps_kpi_num_orders_placed_2' and denum_col == 'zps_kpi_num_purchase_attempts':\n",
    "        ratio_metric_name = 'zps_kpi_PACR'\n",
    "      else:\n",
    "        ratio_metric_name = (num_col+'/'+denum_col)\n",
    "        \n",
    "      results_for_day = pd.Series(data={'experiment_name':experiment_name,'dt':day\n",
    "           ,'metric':ratio_metric_name\n",
    "           ,'metric_type': 'ratio'\n",
    "           ,'mean_control':ratio_control\n",
    "           ,'mean_treatment':ratio_treatment\n",
    "           ,'mean_diff':ratio_diff\n",
    "           ,'mean_control_wo_outliers':np.NaN\n",
    "           ,'mean_treatment_wo_outliers':np.NaN\n",
    "           ,'mean_diff_wo_outliers':np.NaN\n",
    "           ,'median_control': np.NaN\n",
    "           ,'median_treatment':np.NaN\n",
    "           ,'median_diff':np.NaN\n",
    "           ,'ttest_pval':np.NaN\n",
    "           ,'ttest_mean_diff':np.NaN\n",
    "           ,'ttest_ci_prc_low':np.NaN\n",
    "           ,'ttest_ci_prc_up':np.NaN\n",
    "           ,'ttest_wo_outliers_pval':np.NaN\n",
    "           ,'ttest_wo_outliers_mean_diff':np.NaN\n",
    "           ,'ttest_wo_outliers_ci_prc_low':np.NaN\n",
    "           ,'ttest_wo_outliers_ci_prc_up':np.NaN\n",
    "           ,'btstrp_pval':btstrp_pval\n",
    "           ,'btstrp_mean_diff':btstrp_mean_diff\n",
    "           ,'btstrp_ci_prc_low':btstrp_ci_prc_low\n",
    "           ,'btstrp_ci_prc_up':btstrp_ci_prc_up\n",
    "           ,'btstrp_wo_outliers_pval':np.NaN\n",
    "           ,'btstrp_wo_outliers_mean_diff':np.NaN\n",
    "           ,'btstrp_wo_outliers_ci_prc_low':np.NaN\n",
    "           ,'btstrp_wo_outliers_ci_prc_up':np.NaN\n",
    "           ,'buckets_pval':buckets_pval\n",
    "           ,'buckets_mean_diff':buckets_mean_diff\n",
    "           ,'buckets_ci_prc_low':buckets_ci_prc_low\n",
    "           ,'buckets_ci_prc_up':buckets_ci_prc_up\n",
    "           ,'buckets_wo_outliers_pval':np.NaN\n",
    "           ,'buckets_wo_outliers_mean_diff':np.NaN\n",
    "           ,'buckets_wo_outliers_ci_prc_low':np.NaN\n",
    "           ,'buckets_wo_outliers_ci_prc_up':np.NaN\n",
    "           ,'MW_pval':np.NaN\n",
    "           ,'chi2_pval':np.NaN                         \n",
    "           ,'outliers_percentile':np.NaN\n",
    "           ,'outliers_threshold':np.NaN\n",
    "           ,'calculations_time':calculations_time\n",
    "           ,'mde_bootstrap': btstrp_mde\n",
    "           ,'mde_ttest_wo_outliers':np.NaN\n",
    "           ,'units_control': len(control_denum)\n",
    "           ,'units_treatment': len(treatment_denum)\n",
    "           ,'srm_pval': np.round(stats.chisquare([len(control_denum),len(treatment_denum)])[1],4)\n",
    "          },name=calculations_time)\n",
    "      df_results=df_results.append(results_for_day, ignore_index=False)\n",
    "\n",
    "  print(\" %s minutes needed for Tests calculations \" % (np.round((time.time() - start_time)/60,1)))\n",
    "  print(day,'-done')\n",
    "  return df_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da6cb6b-e4e7-40c3-b04b-610dd07a0d50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main_function_for_sample_size_calculations(query, mde, w, treatment_share_percent, pvals, pval_counter, proportions_columns, ratio_columns, continuous_metric_name, continuous_metric_flag, proportions_metric_flag, ratio_metric_flag, continuous_metric_is_median = False):\n",
    "  df_customer = dwhRead(query)\n",
    "  df_customer = df_customer.toPandas()\n",
    "  df_customer= df_customer.sample(frac=treatment_share_percent, replace=False, random_state=1)\n",
    "  s_size=len(df_customer)\n",
    "\n",
    "  \n",
    "  if continuous_metric_flag:\n",
    "    control=df_customer[continuous_metric_name].values\n",
    "    control_uplifted=control*(1+mde)\n",
    "    pval, btstrp_ctr_mean_uplifted, btstrp_tr_mean_uplifted, btstrp_mean_diff_uplifted, btstrp_ci_prc_low_uplifted, btstrp_ci_prc_up_uplifted = get_bootstrap_results(control,control_uplifted,bootstrap_iterations, bootstrap_median = continuous_metric_is_median) \n",
    "    pvals.update({w: pval})\n",
    "    if pval<0.05:\n",
    "      pval_counter+=1\n",
    "    else:\n",
    "      pval_counter=0\n",
    "  \n",
    "  if proportions_metric_flag:\n",
    "    if len(proportions_columns)>1:\n",
    "      raise ValueError('You should input only one proportion metric')\n",
    "    num_name=list(proportions_columns.keys())[0]\n",
    "    denum_name=proportions_columns[num_name]\n",
    "    control_num=df_customer[num_name].values\n",
    "    control_denum=df_customer[denum_name].values\n",
    "    treatment_uplifted_num=np.sum(control_num)*(1+mde)\n",
    "    treatment_uplifted_denum=np.sum(control_denum)\n",
    "    T = np.array([[np.sum(control_num), np.sum(control_denum)-np.sum(control_num)], [treatment_uplifted_num, treatment_uplifted_denum-treatment_uplifted_num]])\n",
    "    pval=np.round(stats.chi2_contingency(T,correction=False)[1],5)\n",
    "    pvals.update({w: pval})\n",
    "    if pval<0.05:\n",
    "      pval_counter+=1\n",
    "    else:\n",
    "      pval_counter=0\n",
    "  \n",
    "  if ratio_metric_flag:\n",
    "    if len(ratio_columns)>1:\n",
    "      raise ValueError('You should input only one ratio metric')\n",
    "    \n",
    "    num_name=list(ratio_columns.keys())[0]\n",
    "    denum_name=ratio_columns[num_name]\n",
    "    control_num=df_customer[num_name].values\n",
    "    control_denum=df_customer[denum_name].values\n",
    "    pval= get_bootstrap_results_for_conversions_mde(control_num, control_denum, mde, bootstrap_iterations)\n",
    "    pvals.update({w: pval})\n",
    "    if pval<0.05:\n",
    "      pval_counter+=1\n",
    "    else:\n",
    "      pval_counter=0\n",
    "    \n",
    "  print ('week=',w,'; pval=',pval)\n",
    "  return pval, w, pval_counter, s_size, pvals\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9196ed6a-8b53-4c5d-be8d-b45e862ef669",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_summary(df_results):\n",
    "  data = df_results[df_results.dt==df_results.dt.max()]\n",
    "  data['pval'] = data[[\"btstrp_pval\", \"chi2_pval\"]].min(axis=1)\n",
    "  return data[['dt','metric', 'metric_type', 'mean_diff', 'pval']].sort_values(by=['pval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc7adbe4-83dc-4f06-b487-82876e8519a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ab_tests_related_functions_v2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
