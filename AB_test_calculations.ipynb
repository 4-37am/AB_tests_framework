{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67faf607-6191-4fb6-bb50-bd2267b9bd27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# What's new in v0.9?\n",
    "\n",
    "# 1) Calculation of stat.tests for medians difference for continuous metrics\n",
    "# 2) Calculation of ZPS KPIs by default (GMVbR, Num Orders, Num Purchase attempts,PACR,PSSR,PPSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951b49d4-a76d-406f-88b1-f6f9b418f577",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /PayDayZPSTeam/_lib/scala/helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db27f7c9-1ae2-463c-b397-a92267e8cc34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %run /PayDayZPSTeam/_lib/python/helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa91a792-bb8a-4d00-bfd2-a1ecf5c9e257",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /../ab_tests_related_functions_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ca1b71-4b4f-451b-8f10-4f46bc9b67b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Input configurations for the experiment calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3343bda-f1bf-4bc5-be1c-ed1e11cb6a99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inputs:\n",
    "\n",
    "experiment_name = 'Test_of_new_feature_for_notebook_repo_fin' # Name of experiment. Try to make it unique\n",
    "test_start_date = '2022-06-10' # First date of roll-out of the experiment\n",
    "calc_start_date = '2022-06-12' # First date of results calculation. Results well be calculate from test_start_date up to date between calc_start_date and calc_end_date.\n",
    "calc_end_date = '2022-06-20' #If you need to calculate results just for 1 day make calc_start_date=calc_end_date\n",
    "# yesterday=(pd.to_datetime(\"today\") - pd.Timedelta(1, unit='D')).strftime('%Y-%m-%d')\n",
    "calc_frequency = '7d' # frequency of calculations between calc_start_date and calc_end_date. Example: if calc_frequency='7d' and calc_start_date ='2022-01-01' and calc_end_date='2022-01-22' then calculations will be performed on: 2022-01-01, 2022-01-08,2022-01-15,2022-01-22. More info on frequency types: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases \n",
    "dates=['2022-07-20', '2022-07-22'] # you can input list of dates to calculate result for each iteratively e.g. for [test_start_date;date_1],[test_start_date,date_2].. if yo want to calculate results daily or by given frequency just leave it empty. Note that number of dates in dates list should be > 2\n",
    "\n",
    "metrics_columns = ['gmvbr','num_purchase_attempts','num_orders',{'gmvbr':'median'}] # Names of metrics you need to calculate stat.tests. Should be the same as names in sql query results table.\n",
    "\n",
    "proportions_metrics_flag = True # if you need to calculate proportion metrics in 'numerator/denumerator' format\n",
    "proportions_columns = {'num_customers_with_morethan2_orders':'all_customers'} # names of metrics from sql query that you want to use for proportion metrics in numerator:denumerator format. Note that values per user must be binary 0 or 1. \n",
    "\n",
    "ratio_metrics_flag = True # if you need to calculate ratio metrics in 'numerator/denumerator' format\n",
    "ratio_columns = {'num_orders':'num_purchase_attempts'} # names of metrics from sql query that you want to use for ratio metrics in numerator:denumerator format. \n",
    "\n",
    "need_bootstrap = True # True if you want to calculate Bootstrap statistics \n",
    "need_boostrap_wo_outliers = True # True if you want to calculate Bootstrap statistics based on data with removed outliers\n",
    "need_buckets = False # True if you want to calculate ttest based on buckets split\n",
    "need_buckets_wo_outliers = False # True if you want to calculate ttest based on buckets split of data with removed outliers\n",
    "is_zps_kpis_list_needed = True # True if you want to get ZPS KPIs (GMV,PACR,PSSR,PPSR) calculated by default\n",
    "\n",
    "quantile_for_outliers = 0.99999 # quantile for outliers removing. Users who have metric value above this quentile will be removed for 'without outliers' calculations\n",
    "bootstrap_iterations = 1000 # number of iterations in bootstrap. You can \n",
    "buckets_num = 150 # number of buckets to split (more buckets -> less customers in one bucket -> less precision; less buckets -> less sample for ttest -> less precision, so it is always a tradeoff. Recomendation is not to use less than 60 buckets and have number of customers in bucket not less than 4000)\n",
    "\n",
    "customer_id_column = 'customer_id' #name of column in sql resulted table that contains customer IDs\n",
    "variant_col = 'payload_variant' #name of column in sql resulted table that contains names of treatment and control groups\n",
    "control_gr_name = 'Invoice' #name of control group in variant_col\n",
    "treatment_gr_name = 'BNPL2.0' #name of treatment group in variant_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa702059-d3f0-41aa-91b9-4cab421d37cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Input name of table where rusults should be placed (table should be created before the start of calculations. if not - uncomment and run cmd 7)\n",
    "val TargetTable = \"zps_analytics.ab_test_calculations_new_approach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df69e1c1-28ad-4458-b38d-784a9cbeb29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// create table for stat.tests results collection on RedShift if not yet created:\n",
    "\n",
    "// val TargetTable = \"zps_analytics.ab_test_calculations_new_approach\"\n",
    "\n",
    "// dwhSend(s\"\"\"\n",
    "//   -- Want to rebuild the full table? Then uncomment the next line. :)\n",
    "//   drop table if exists $TargetTable;\n",
    "//   create table if not exists $TargetTable (\n",
    "//        experiment_name varchar(255)\n",
    "//        ,dt date\n",
    "//        ,metric varchar(255)\n",
    "//        ,metric_type varchar(255)\n",
    "//        ,mean_control float\n",
    "//        ,mean_treatment float\n",
    "//        ,mean_diff float\n",
    "//        ,mean_control_wo_outliers float\n",
    "//        ,mean_treatment_wo_outliers float\n",
    "//        ,mean_diff_wo_outliers float\n",
    "//        ,median_control float\n",
    "//        ,median_treatment float\n",
    "//        ,median_diff float\n",
    "//        ,ttest_pval float\n",
    "//        ,ttest_mean_diff float\n",
    "//        ,ttest_ci_prc_low float\n",
    "//        ,ttest_ci_prc_up float\n",
    "//        ,ttest_wo_outliers_pval float\n",
    "//        ,ttest_wo_outliers_mean_diff float\n",
    "//        ,ttest_wo_outliers_ci_prc_low float\n",
    "//        ,ttest_wo_outliers_ci_prc_up float\n",
    "//        ,btstrp_pval float\n",
    "//        ,btstrp_mean_diff float\n",
    "//        ,btstrp_ci_prc_low float\n",
    "//        ,btstrp_ci_prc_up float\n",
    "//        ,btstrp_wo_outliers_pval float\n",
    "//        ,btstrp_wo_outliers_mean_diff float\n",
    "//        ,btstrp_wo_outliers_ci_prc_low float\n",
    "//        ,btstrp_wo_outliers_ci_prc_up float\n",
    "//        ,buckets_pval float\n",
    "//        ,buckets_mean_diff float\n",
    "//        ,buckets_ci_prc_low float\n",
    "//        ,buckets_ci_prc_up float\n",
    "//        ,buckets_wo_outliers_pval float\n",
    "//        ,buckets_wo_outliers_mean_diff float\n",
    "//        ,buckets_wo_outliers_ci_prc_low float\n",
    "//        ,buckets_wo_outliers_ci_prc_up float\n",
    "//        ,MW_pval float\n",
    "//        ,chi2_pval float                        \n",
    "//        ,outliers_percentile float\n",
    "//        ,outliers_threshold float\n",
    "//        ,calculations_time varchar(255)\n",
    "//        ,mde_bootstrap float\n",
    "//        ,mde_ttest_wo_outliers float\n",
    "//        ,units_control float\n",
    "//        ,units_treatment float\n",
    "//        ,srm_pval float\n",
    "//   );\n",
    "  \n",
    "//   -- share with all Redshift users (mark as comment if not wished)\n",
    "//   grant select on $TargetTable to public\n",
    "// \"\"\")\n",
    "\n",
    "//  // if you need to add new column:\n",
    "// val sqlAdd = s\"ALTER TABLE $TargetTable ADD COLUMN srm_pval float DEFAULT Null\"\n",
    "// dwhSend(sqlAdd)\n",
    "\n",
    "// if you need to remove rows:\n",
    "// val sqlDelete = s\"DELETE FROM $TargetTable WHERE experiment_name = 'Test_of_new_feature_for_notebook_2'  \"\n",
    "// dwhSend(sqlDelete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9be37fe-0632-413e-9a4c-545a961412f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Insert SQL query in cmd9 that counts needed metrics grouped by customer_id and variant type. \n",
    "Use date parameters from config ('test_start_date' and 'day' as date between calc_start_date and calc_end_date):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b43de0a-b6c5-452a-9d49-4d510104008e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results=pd.DataFrame()\n",
    "  # create a list of dates for calculations\n",
    "if len(dates)<3:\n",
    "  dates=pd.date_range(calc_start_date,calc_end_date,freq=calc_frequency).strftime('%Y-%m-%d').tolist()\n",
    "calculations_time=pd.to_datetime(\"today\").strftime('%Y-%m-%d-%H-%M-%S')\n",
    "print(dates)\n",
    "for day in dates:\n",
    "\n",
    "#  Actions needed!\n",
    "#   insert your query with data grouped by customer_id and group type (control/treatment). Add date parameters for test_start_date and day of calculations.\n",
    "    \n",
    "    query= f\"\"\"\n",
    "with test_assignment as (\n",
    "        --group assignment info from octopus\n",
    "        select\n",
    "            customer_number as customer_id\n",
    "            , min(payload_variant) as payload_variant\n",
    "            , min(occurred_at) as assigned_at\n",
    "            from ods.ods_f_octopus_feedback\n",
    "        where payload_experiment_id in (\n",
    "            'f1e6574c-a5a5-4f51-be73-c472e3bc91b9'\n",
    "            , 'b9b3b401-da13-434f-b970-224ce3b9c01e'\n",
    "        )  --experiment_id for BNPL DE\n",
    "         and payload_mode like 'TEST'\n",
    "        group by 1\n",
    "\n",
    "    )\n",
    "     select ta.customer_id\n",
    "         , ta.payload_variant\n",
    "         , coalesce(sum(gmv_bef_cancellation), 0)::float as gmvbr\n",
    "         , coalesce(count(purchase_attempt_id), 0) as num_purchase_attempts\n",
    "         , coalesce(sum(num_orders_placed), 0) as num_orders\n",
    "         , 1 as all_customers\n",
    "         , case when num_orders>1 then 1 else 0 end as num_customers_with_morethan2_orders\n",
    "    from reporting.pam_purchase_attempt_funnel cm\n",
    "    inner join test_assignment as ta using(customer_id)\n",
    "    where purchase_attempt_created_at::date between '\"\"\"+test_start_date+\"\"\"' and '\"\"\"+day+\"\"\"' and sales_channel='Shop DE'\n",
    "    group by 1,2\n",
    "\n",
    "\"\"\"\n",
    "    query_flat = None  #you can write here additional query, for more information https://docs.google.com/document/d/1jh4f5k_n13ioJTQFUKj1eX0eEpKiKVXnacj35fmm1oI/edit#heading=h.z71pmqeqe5dw\n",
    "    df_day_results = main_ab_test_calculation_function(customer_id_column, test_start_date, day, calc_start_date, calc_end_date, metrics_columns, need_bootstrap, need_buckets, proportions_metrics_flag, proportions_columns, ratio_metrics_flag, ratio_columns, query, is_zps_kpis_list_needed, query_flat=query_flat)\n",
    "    df_results = df_results.append(df_day_results, ignore_index=False)\n",
    "\n",
    "df_results = df_results.sort_values(by=['metric'])\n",
    "df_to_redshift = spark.createDataFrame(df_results)\n",
    "df_to_redshift.createOrReplaceTempView(\"tmp\")\n",
    "df_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318f276a-b1a0-4bb1-a61c-90e9047257a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// change nothing here, just run it\n",
    "var df_to_redshift_final = sql(\"\"\"\n",
    "select      \n",
    "   experiment_name::string\n",
    "   ,cast(dt as date) dt\n",
    "   ,metric::string\n",
    "   ,metric_type::string\n",
    "   ,mean_control::double\n",
    "   ,mean_treatment::double\n",
    "   ,mean_diff::double\n",
    "   ,mean_control_wo_outliers::double\n",
    "   ,mean_treatment_wo_outliers::double\n",
    "   ,mean_diff_wo_outliers::double\n",
    "   ,median_control::double\n",
    "   ,median_treatment::double\n",
    "   ,median_diff::double\n",
    "   ,ttest_pval::double\n",
    "   ,ttest_mean_diff::double\n",
    "   ,ttest_ci_prc_low::double\n",
    "   ,ttest_ci_prc_up::double\n",
    "   ,ttest_wo_outliers_pval::double\n",
    "   ,ttest_wo_outliers_mean_diff::double\n",
    "   ,ttest_wo_outliers_ci_prc_low::double\n",
    "   ,ttest_wo_outliers_ci_prc_up::double\n",
    "   ,btstrp_pval::double\n",
    "   ,btstrp_mean_diff::double\n",
    "   ,btstrp_ci_prc_low::double\n",
    "   ,btstrp_ci_prc_up::double\n",
    "   ,btstrp_wo_outliers_pval::double\n",
    "   ,btstrp_wo_outliers_mean_diff::double\n",
    "   ,btstrp_wo_outliers_ci_prc_low::double\n",
    "   ,btstrp_wo_outliers_ci_prc_up::double\n",
    "   ,buckets_pval::double\n",
    "   ,buckets_mean_diff::double\n",
    "   ,buckets_ci_prc_low::double\n",
    "   ,buckets_ci_prc_up::double\n",
    "   ,buckets_wo_outliers_pval::double\n",
    "   ,buckets_wo_outliers_mean_diff::double\n",
    "   ,buckets_wo_outliers_ci_prc_low::double\n",
    "   ,buckets_wo_outliers_ci_prc_up::double\n",
    "   ,MW_pval::double\n",
    "   ,chi2_pval::double                        \n",
    "   ,outliers_percentile::double\n",
    "   ,outliers_threshold::double\n",
    "   ,calculations_time::string\n",
    "   ,mde_bootstrap::double\n",
    "   ,mde_ttest_wo_outliers::double\n",
    "   ,units_control::double\n",
    "   ,units_treatment::double\n",
    "   ,srm_pval::double\n",
    "from tmp\"\"\")\n",
    "display(df_to_redshift_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "347f46c4-4ff1-47c4-85f5-a16b88060681",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_summary(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f30decb-6bc9-451f-a542-efc729410bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "//  write tests results in Redshift TargetTable defined in cmd 6 \n",
    "val sqlPreRun = s\"select * from $TargetTable\"\n",
    "dwhWrite(df_to_redshift_final, TargetTable, sqlPreRun)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "AB_test_calculations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
